{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_images_with_labels(data_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(data_folder):\n",
    "        folder_path = os.path.join(data_folder, label)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                if os.path.isfile(img_path) and img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "                    img = img.resize((80, 80))  # Resize to 80x80\n",
    "                    images.append(np.array(img))\n",
    "                    labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinhala_letter_id_cnn():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(80, 80, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),  # Added another conv layer\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),  # Increased units for the dense layer\n",
    "        layers.Dropout(0.5),  # Added dropout\n",
    "        layers.Dense(244, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your data folder\n",
    "train_data_folder = 'C:/Users/cmsmc/Desktop/Projects/SinhalaTranslator/Work/Dataset454/train'\n",
    "validation_data_folder = 'C:/Users/cmsmc/Desktop/Projects/SinhalaTranslator/Work/Dataset454/valid'\n",
    "test_data_folder = 'C:/Users/cmsmc/Desktop/Projects/SinhalaTranslator/Work/Dataset454/test'\n",
    "\n",
    "# Load the data \n",
    "training_images, training_labels = load_images_with_labels(train_data_folder)\n",
    "validation_images, validation_labels = load_images_with_labels(validation_data_folder)\n",
    "test_images, test_labels = load_images_with_labels(test_data_folder)\n",
    "\n",
    "# Normalize image data\n",
    "training_images = training_images / 255.0\n",
    "validation_images = validation_images / 255.0\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_training_labels = label_encoder.fit_transform(training_labels)\n",
    "encoded_validation_labels = label_encoder.transform(validation_labels) \n",
    "\n",
    "# Reshape image data to add a channel dimension\n",
    "training_images = np.expand_dims(training_images, axis=-1)\n",
    "validation_images = np.expand_dims(validation_images, axis=-1)\n",
    "\n",
    "# Check for NumPy version compatibility\n",
    "if not hasattr(np, 'complex_'):\n",
    "    np.complex_ = np.complex128\n",
    "\n",
    "# Create an instance of the CNN.\n",
    "sinhala_letter_cnn = create_sinhala_letter_id_cnn()\n",
    "\n",
    "# Compile the model.\n",
    "sinhala_letter_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary.\n",
    "sinhala_letter_cnn.summary()\n",
    "\n",
    "# Train the model.\n",
    "history = sinhala_letter_cnn.fit(training_images, encoded_training_labels, epochs=40, validation_data=(validation_images, encoded_validation_labels))\n",
    "\n",
    "# Save the trained model\n",
    "sinhala_letter_cnn.save('./TrainedModels/CNN_244_trial_1.keras')\n",
    "\n",
    "# Assuming you have `training_labels` from your dataset\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_training_labels = label_encoder.fit_transform(training_labels)\n",
    "\n",
    "# Save the classes\n",
    "np.save('./TrainedModels/CNN_244_trial_1_classes.npy', label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function to plot training and validation metrics\n",
    "def plot_metrics(history):\n",
    "\n",
    "    # Extracting values from the history object\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Number of epochs\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plotting training and validation accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have already trained your model and have the history object\n",
    "plot_metrics(history)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "\n",
    "def load_images_with_labels(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(folder_path):\n",
    "        label_folder = os.path.join(folder_path, label)\n",
    "        for image_name in os.listdir(label_folder):\n",
    "            image_path = os.path.join(label_folder, image_name)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Path to the test data folder\n",
    "test_data_folder = 'C:/Users/cmsmc/Desktop/Projects/SinhalaTranslator/Work/Dataset454/test'\n",
    "\n",
    "# Load the test images and labels\n",
    "test_images, test_labels = load_images_with_labels(test_data_folder)\n",
    "test_images = test_images / 255.0  # Normalize the images\n",
    "test_images = np.expand_dims(test_images, axis=-1)  # Add the channel dimension\n",
    "\n",
    "# Load the label encoder used during training\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Load the classes used during training\n",
    "label_encoder.classes_ = np.load('./TrainedModels/CNN_244_trial_1_classes.npy') \n",
    "encoded_test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = load_model('./TrainedModels/CNN_244_trial_1.keras')\n",
    "\n",
    "# Perform evaluation\n",
    "predictions = model.predict(test_images)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(encoded_test_labels, predicted_labels)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(encoded_test_labels, predicted_labels, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from itertools import product  # For generating all combinations of hyperparameters\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "conv_filters = [(32, 64), (64, 128)]   # Different filter sizes for each conv layer\n",
    "dense_units = [64, 128]                # Units in the dense layer\n",
    "dropout_rates = [0.3, 0.5]             # Different dropout rates\n",
    "learning_rates = [0.001, 0.0001]       # Learning rates to test\n",
    "batch_sizes = [32, 64]                 # Batch sizes to try\n",
    "\n",
    "# Create directory to save models and configurations\n",
    "output_dir = \"./trained_models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to create a model with specific hyperparameters\n",
    "def create_model(conv_filter1, conv_filter2, dense_unit, dropout_rate, learning_rate):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(80, 80, 1)),\n",
    "        layers.Conv2D(conv_filter1, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(conv_filter2, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dense_unit, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(244, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with given learning rate\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for conv_filter1, conv_filter2, dense_unit, dropout_rate, learning_rate, batch_size in product(\n",
    "    conv_filters, dense_units, dropout_rates, learning_rates, batch_sizes\n",
    "):\n",
    "    # Create model with specific parameters\n",
    "    model = create_model(conv_filter1, conv_filter2, dense_unit, dropout_rate, learning_rate)\n",
    "\n",
    "    # Define model save path\n",
    "    model_name = f\"model_filters_{conv_filter1}_{conv_filter2}_dense_{dense_unit}_dropout_{dropout_rate}_lr_{learning_rate}_batch_{batch_size}\"\n",
    "    model_path = os.path.join(output_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    # Save the model's architecture as JSON\n",
    "    with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "        config = {\n",
    "            \"conv_filter1\": conv_filter1,\n",
    "            \"conv_filter2\": conv_filter2,\n",
    "            \"dense_units\": dense_unit,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size\n",
    "        }\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    # Train the model (you can add callbacks such as EarlyStopping if needed)\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(\n",
    "        train_data,  # Replace with your actual training data\n",
    "        train_labels,\n",
    "        epochs=10,  # Adjust as needed\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(val_data, val_labels),  # Replace with your actual validation data\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Save the trained model weights\n",
    "    model.save(os.path.join(model_path, \"model.h5\"))\n",
    "\n",
    "print(\"Training and model saving complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
